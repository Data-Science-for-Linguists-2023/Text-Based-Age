{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit dataset analysis\n",
    "\n",
    "Here's the plan for the upcoming work in the notebook --\n",
    "\n",
    "- CountVectorizer (vector representation of text) on N-grams (vary the N)\n",
    "- Train a Naive Bayes Classifier\n",
    "- Look at most significant features\n",
    "- Iteratively improve feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What happened to my comment....it was soo good...</td>\n",
       "      <td>te</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A shit ton of censorship. And I don't mean \"de...</td>\n",
       "      <td>te</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wasn't aware of the drama between /r/askmen an...</td>\n",
       "      <td>te</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nice username  I too am from Finland</td>\n",
       "      <td>te</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Your comment was on the [other post]( lol</td>\n",
       "      <td>te</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64861</th>\n",
       "      <td>And, after 10 years of marriage, you can get 5...</td>\n",
       "      <td>th</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64862</th>\n",
       "      <td>Yes. Thank you for this response. I don‚Äôt view...</td>\n",
       "      <td>th</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64863</th>\n",
       "      <td>Better hope that you're contacted before someo...</td>\n",
       "      <td>th</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64864</th>\n",
       "      <td>Thank you for this question. I also find mysel...</td>\n",
       "      <td>th</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64865</th>\n",
       "      <td>Yes. Being able to consider working part-time ...</td>\n",
       "      <td>th</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64866 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text age\n",
       "0      What happened to my comment....it was soo good...  te\n",
       "1      A shit ton of censorship. And I don't mean \"de...  te\n",
       "2      Wasn't aware of the drama between /r/askmen an...  te\n",
       "3                   Nice username  I too am from Finland  te\n",
       "4              Your comment was on the [other post]( lol  te\n",
       "...                                                  ...  ..\n",
       "64861  And, after 10 years of marriage, you can get 5...  th\n",
       "64862  Yes. Thank you for this response. I don‚Äôt view...  th\n",
       "64863  Better hope that you're contacted before someo...  th\n",
       "64864  Thank you for this question. I also find mysel...  th\n",
       "64865  Yes. Being able to consider working part-time ...  th\n",
       "\n",
       "[64866 rows x 2 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "loaded_df = pd.read_pickle('../../data_samples/reddit_samples/all.pkl')\n",
    "loaded_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def train_classifier(texts, labels):\n",
    "    # create a CountVectorizer with character-based bigrams\n",
    "    vectorizer = CountVectorizer(analyzer='char', ngram_range=(2,2), lowercase=False, stop_words=None)\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    # train the classifier and return it\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(X, labels)\n",
    "    return clf, vectorizer\n",
    "\n",
    "def train_word_classifier(texts, labels):\n",
    "    # create a CountVectorizer with words\n",
    "    vectorizer = CountVectorizer(lowercase=False, stop_words=None)\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    # train the classifier and return it\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(X, labels)\n",
    "    return clf, vectorizer\n",
    "\n",
    "def predict_age_group(classifier, vectorizer, new_text):\n",
    "    # take in a classifier as input and return the prediction\n",
    "    new_X = vectorizer.transform([new_text])\n",
    "    predicted_age_group = classifier.predict(new_X)\n",
    "    return predicted_age_group\n",
    "\n",
    "def evaluate_classifier(classifier, vectorizer, test_texts, test_labels):\n",
    "    # transform the test data\n",
    "    X_test = vectorizer.transform(test_texts)\n",
    "    # predict the age group and return score\n",
    "    predicted_age_groups = classifier.predict(X_test)\n",
    "    return accuracy_score(test_labels, predicted_age_groups)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6040542623708957\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(list(loaded_df['text']), list(loaded_df['age']), test_size=0.2)\n",
    "\n",
    "# Train a classifier on the training data\n",
    "clf, vectorizer = train_classifier(train_texts, train_labels)\n",
    "\n",
    "# Evaluate the classifier on the test data\n",
    "accuracy = evaluate_classifier(clf, vectorizer, test_texts, test_labels)\n",
    "\n",
    "# Print the accuracy score\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most significant textual features:\n",
      "['ÕëÕ†', 'er', 'k‚ñë', 'wi', 'u\\\\', '‚Ä†h', ' üíö', 'ÃéÕë', 'ü•µ ', 've', 'ÕëÕ†', 'er', 'k‚ñë', 'wi', 'u\\\\', '‚Ä†h', 'ü•µ ', 'ÃéÕë', ' üíö', 'ùïåœÇ', 'ÕëÕ†', 'er', 'k‚ñë', 'wi', '‚Ä†h', 'u\\\\', 'ü•µ ', 'ÃéÕë', ' üíö', '‚Çú ']\n"
     ]
    }
   ],
   "source": [
    "N = 10\n",
    "feature_names = list(vectorizer.vocabulary_.keys())\n",
    "log_prob = clf.feature_log_prob_\n",
    "top_N_features = []\n",
    "for i in range(clf.classes_.shape[0]):\n",
    "    top_N_indices = log_prob[i].argsort()[::-1][:N]\n",
    "    top_N_features.extend([feature_names[idx] for idx in top_N_indices])\n",
    "\n",
    "print(\"Top {} most significant textual features:\".format(N))\n",
    "print(top_N_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6983197163557885\n"
     ]
    }
   ],
   "source": [
    "# Train a classifier on the training data\n",
    "word_clf, word_vectorizer = train_word_classifier(train_texts, train_labels)\n",
    "\n",
    "# Evaluate the classifier on the test data\n",
    "accuracy = evaluate_classifier(word_clf, word_vectorizer, test_texts, test_labels)\n",
    "\n",
    "# Print the accuracy score\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most significant textual features:\n",
      "['raisins', 'fostered', 'unexpectednaruto', 'irreversible', 'Shel', 'itty', 'Hitting', 'KING', 'Newtech', 'goddammit', 'raisins', 'fostered', 'unexpectednaruto', 'Shel', 'Hitting', 'irreversible', 'itty', 'KING', 'goddammit', 'Else', 'raisins', 'fostered', 'unexpectednaruto', 'Shel', 'irreversible', 'Hitting', 'goddammit', 'itty', 'KING', 'Else']\n"
     ]
    }
   ],
   "source": [
    "N = 10\n",
    "feature_names = list(word_vectorizer.vocabulary_.keys())\n",
    "log_prob = word_clf.feature_log_prob_\n",
    "top_N_features = []\n",
    "for i in range(clf.classes_.shape[0]):\n",
    "    top_N_indices = log_prob[i].argsort()[::-1][:N]\n",
    "    top_N_features.extend([feature_names[idx] for idx in top_N_indices])\n",
    "\n",
    "print(\"Top {} most significant textual features:\".format(N))\n",
    "print(top_N_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train_ensemble_classifier(texts, labels):\n",
    "    # train the character-level bigram model\n",
    "    clf_char, vec_char = train_classifier(texts, labels)\n",
    "\n",
    "    # train the word-level model\n",
    "    clf_word, vec_word = train_word_classifier(texts, labels)\n",
    "\n",
    "    # get the predicted probabilities of the base classifiers\n",
    "    X_char = vec_char.transform(texts)\n",
    "    X_word = vec_word.transform(texts)\n",
    "    proba_char = clf_char.predict_proba(X_char)\n",
    "    proba_word = clf_word.predict_proba(X_word)\n",
    "\n",
    "    # horizontally stack the predicted probabilities\n",
    "    proba_combined = np.hstack([proba_char, proba_word])\n",
    "\n",
    "    # train the meta-classifier\n",
    "    meta_clf = LogisticRegression()\n",
    "    meta_clf.fit(proba_combined, labels)\n",
    "\n",
    "    return clf_char, clf_word, meta_clf, vec_char, vec_word\n",
    "\n",
    "\n",
    "def evaluate_ensemble_classifier(clf_char, clf_word, meta_clf, vec_char, vec_word, test_texts, test_labels):\n",
    "    # transform the test data\n",
    "    X_char = vec_char.transform(test_texts)\n",
    "    X_word = vec_word.transform(test_texts)\n",
    "\n",
    "    # get the predicted probabilities of the base classifiers\n",
    "    proba_char = clf_char.predict_proba(X_char)\n",
    "    proba_word = clf_word.predict_proba(X_word)\n",
    "\n",
    "    # horizontally stack the predicted probabilities\n",
    "    proba_combined = np.hstack([proba_char, proba_word])\n",
    "\n",
    "    # predict the labels using the meta-classifier\n",
    "    predicted_labels = meta_clf.predict(proba_combined)\n",
    "\n",
    "    return accuracy_score(test_labels, predicted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 70.33%\n"
     ]
    }
   ],
   "source": [
    "# train the ensemble classifier\n",
    "clf_char, clf_word, meta_clf, vec_char, vec_word = train_ensemble_classifier(train_texts, train_labels)\n",
    "\n",
    "# preprocess the test data using the vectorizers and evaluate the classifier\n",
    "accuracy = evaluate_ensemble_classifier(clf_char, clf_word, meta_clf, vec_char, vec_word, test_texts, test_labels)\n",
    "\n",
    "# print the accuracy score\n",
    "print('Accuracy: {:.2f}%'.format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "feature_names = list(word_vectorizer.vocabulary_.keys())\n",
    "log_prob = word_clf.feature_log_prob_\n",
    "top_N_features = []\n",
    "for i in range(clf.classes_.shape[0]):\n",
    "    top_N_indices = log_prob[i].argsort()[::-1][:N]\n",
    "    top_N_features.extend([feature_names[idx] for idx in top_N_indices])\n",
    "\n",
    "print(\"Top {} most significant textual features:\".format(N))\n",
    "print(top_N_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['That‚Äôs horrible for him to say something like that, It‚Äôs good that you aren‚Äôt friends with him anymore. Also, him saying that he is average weight for Americans isn‚Äôt a good thing, average weight for Americans is likely obese, or at best overweight.',\n",
       " 'Wow, I missed this in my first read through. The way he‚Äôs treating OP is awful, but this is actually horrifying and the clearest indicator that this dude is a bad, bad person.',\n",
       " \"Not op but Canvas is awesome. It connects directly to Google drive, it's straightforward, it's well designed, AND it has a usable app. A+\",\n",
       " 'What the fuck do you guys do if there is a nonbinary',\n",
       " 'the first comment was great, also when she is honest like says she feels unsafe or having lots of anxiety def take her out of the situation, if she goes out always pick her up when she asks (could get her out of an anxiety driven situation)']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def preprocess_texts(texts):\n",
    "    preprocessed_texts = []\n",
    "    \n",
    "    # create stop words list and instantiate stemmer and lemmatizer\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    for text in texts:\n",
    "        # lowercase the text\n",
    "        text = text.lower()\n",
    "        \n",
    "        # remove URLs and email addresses\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+|ftp\\S+|@\\S+', '', text, flags=re.MULTILINE)\n",
    "        \n",
    "        # remove non-alphanumeric characters except for spaces\n",
    "        text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "        \n",
    "        # remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        # tokenize the text\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # remove stop words\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        \n",
    "        # stem the tokens\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "        \n",
    "        # lemmatize the tokens\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        \n",
    "        # re-join the tokens into a single string\n",
    "        preprocessed_text = ' '.join(tokens)\n",
    "        \n",
    "        preprocessed_texts.append(preprocessed_text)\n",
    "    \n",
    "    return preprocessed_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = preprocess_texts(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_test = preprocess_texts(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 69.23%\n"
     ]
    }
   ],
   "source": [
    "# train the ensemble classifier\n",
    "clf_char, clf_word, meta_clf, vec_char, vec_word = train_ensemble_classifier(pre, train_labels)\n",
    "\n",
    "# preprocess the test data using the vectorizers and evaluate the classifier\n",
    "accuracy = evaluate_ensemble_classifier(clf_char, clf_word, meta_clf, vec_char, vec_word, pre_test, test_labels)\n",
    "\n",
    "# print the accuracy score\n",
    "print('Accuracy: {:.2f}%'.format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6933867735470942\n"
     ]
    }
   ],
   "source": [
    "# Train a classifier on the training data\n",
    "word_clf, word_vectorizer = train_word_classifier(pre, train_labels)\n",
    "\n",
    "# Evaluate the classifier on the test data\n",
    "accuracy = evaluate_classifier(word_clf, word_vectorizer, pre_test, test_labels)\n",
    "\n",
    "# Print the accuracy score\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5701402805611222\n"
     ]
    }
   ],
   "source": [
    "# Train a classifier on the training data\n",
    "clf, vectorizer = train_classifier(pre, train_labels)\n",
    "\n",
    "# Evaluate the classifier on the test data\n",
    "accuracy = evaluate_classifier(clf, vectorizer, pre_test, test_labels)\n",
    "\n",
    "# Print the accuracy score\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def analyze_sentence_length(text):\n",
    "    \"\"\"\n",
    "    tokenizes into sentences and returns list of lengths\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    sentence_lengths = [len(sent) for sent in doc.sents]\n",
    "    return sentence_lengths\n",
    "\n",
    "\n",
    "def analyze_words(text):\n",
    "    \"\"\"\n",
    "    tokenizes into words and returns the vocabulary size\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct and not token.is_space]\n",
    "    vocabulary_size = len(set(tokens))\n",
    "    return vocabulary_size\n",
    "\n",
    "\n",
    "def analyze_syntax(text):\n",
    "    \"\"\"\n",
    "    tokenizes the given text into words\n",
    "    applies POS tagging\n",
    "    returns a dictionary containing frequency count of each tag\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    tag_counts = Counter([token.pos_ for token in doc if not token.is_stop and not token.is_punct and not token.is_space])\n",
    "    return tag_counts\n",
    "\n",
    "\n",
    "def analyze_caps(text):\n",
    "    \"\"\"\n",
    "    tokenizes into words and identifies capitalized words\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    capitalized_words = [token.text for token in doc if token.text.isupper() and not token.is_stop and not token.is_punct and not token.is_space]\n",
    "    return capitalized_words\n",
    "\n",
    "\n",
    "def word_freqs(text):\n",
    "    \"\"\"\n",
    "    tokenizes into words and counts the frequency of each word\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    words = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct and not token.is_space]\n",
    "    word_frequencies = dict(Counter(words))\n",
    "    return word_frequencies\n",
    "\n",
    "\n",
    "def analyze(text):\n",
    "    \"\"\"\n",
    "    returns everything\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"sentence_lengths\": analyze_sentence_length(text),\n",
    "        \"vocabulary_size\": analyze_words(text),\n",
    "        \"pos_tag_counts\": analyze_syntax(text),\n",
    "        \"capitalized_words\": analyze_caps(text),\n",
    "        \"word_frequencies\": word_freqs(text)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Unsupported type <class 'int'> in iterable value. Only iterables of string are supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 27\u001b[0m\n\u001b[1;32m     21\u001b[0m pipeline \u001b[39m=\u001b[39m Pipeline([\n\u001b[1;32m     22\u001b[0m     (\u001b[39m'\u001b[39m\u001b[39mextract_features\u001b[39m\u001b[39m'\u001b[39m, extract_features),\n\u001b[1;32m     23\u001b[0m     (\u001b[39m'\u001b[39m\u001b[39mvectorize\u001b[39m\u001b[39m'\u001b[39m, DictVectorizer()),\n\u001b[1;32m     24\u001b[0m ])\n\u001b[1;32m     26\u001b[0m \u001b[39m# Apply the pipeline to the training data to extract and vectorize the features\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m X_train \u001b[39m=\u001b[39m pipeline\u001b[39m.\u001b[39;49mfit_transform(train_texts[\u001b[39m0\u001b[39;49m:\u001b[39m10000\u001b[39;49m])\n\u001b[1;32m     28\u001b[0m y_train \u001b[39m=\u001b[39m train_labels[\u001b[39m0\u001b[39m:\u001b[39m10000\u001b[39m]\n\u001b[1;32m     30\u001b[0m \u001b[39m# Print the feature matrix\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/pipeline.py:445\u001b[0m, in \u001b[0;36mPipeline.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    443\u001b[0m fit_params_last_step \u001b[39m=\u001b[39m fit_params_steps[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]]\n\u001b[1;32m    444\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(last_step, \u001b[39m\"\u001b[39m\u001b[39mfit_transform\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 445\u001b[0m     \u001b[39mreturn\u001b[39;00m last_step\u001b[39m.\u001b[39;49mfit_transform(Xt, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_last_step)\n\u001b[1;32m    446\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m     \u001b[39mreturn\u001b[39;00m last_step\u001b[39m.\u001b[39mfit(Xt, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_last_step)\u001b[39m.\u001b[39mtransform(Xt)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/feature_extraction/_dict_vectorizer.py:312\u001b[0m, in \u001b[0;36mDictVectorizer.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Learn a list of feature name -> indices mappings and transform X.\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \n\u001b[1;32m    291\u001b[0m \u001b[39mLike fit(X) followed by transform(X), but does not require\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[39m    Feature vectors; always 2-d.\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m--> 312\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transform(X, fitting\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/feature_extraction/_dict_vectorizer.py:230\u001b[0m, in \u001b[0;36mDictVectorizer._transform\u001b[0;34m(self, X, fitting)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(v, Mapping) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(v, Iterable):\n\u001b[1;32m    229\u001b[0m     feature_name \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 230\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_add_iterable_element(\n\u001b[1;32m    231\u001b[0m         f,\n\u001b[1;32m    232\u001b[0m         v,\n\u001b[1;32m    233\u001b[0m         feature_names,\n\u001b[1;32m    234\u001b[0m         vocab,\n\u001b[1;32m    235\u001b[0m         fitting\u001b[39m=\u001b[39;49mfitting,\n\u001b[1;32m    236\u001b[0m         transforming\u001b[39m=\u001b[39;49mtransforming,\n\u001b[1;32m    237\u001b[0m         indices\u001b[39m=\u001b[39;49mindices,\n\u001b[1;32m    238\u001b[0m         values\u001b[39m=\u001b[39;49mvalues,\n\u001b[1;32m    239\u001b[0m     )\n\u001b[1;32m    240\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    241\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    242\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnsupported value Type \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(v)\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfor \u001b[39m\u001b[39m{\u001b[39;00mf\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mv\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    244\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(v)\u001b[39m}\u001b[39;00m\u001b[39m objects are not supported.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    245\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/feature_extraction/_dict_vectorizer.py:122\u001b[0m, in \u001b[0;36mDictVectorizer._add_iterable_element\u001b[0;34m(self, f, v, feature_names, vocab, fitting, transforming, indices, values)\u001b[0m\n\u001b[1;32m    120\u001b[0m     vv \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    121\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 122\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    123\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnsupported type \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(vv)\u001b[39m}\u001b[39;00m\u001b[39m in iterable \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mvalue. Only iterables of string are \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    125\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msupported.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    126\u001b[0m     )\n\u001b[1;32m    127\u001b[0m \u001b[39mif\u001b[39;00m fitting \u001b[39mand\u001b[39;00m feature_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m vocab:\n\u001b[1;32m    128\u001b[0m     vocab[feature_name] \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(feature_names)\n",
      "\u001b[0;31mTypeError\u001b[0m: Unsupported type <class 'int'> in iterable value. Only iterables of string are supported."
     ]
    }
   ],
   "source": [
    "analyze(\"hi my name is varun\")\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Define a function to extract the features from a single document\n",
    "def analyze_single(text):\n",
    "    return {\n",
    "        'sentence_lengths': [len(sent) for sent in nlp(text).sents],\n",
    "        'vocabulary_size': len(set(nlp(text).text.lower().split())),\n",
    "        'pos_tag_counts': [token.pos_ for token in nlp(text)],\n",
    "        'capitalized_words': [token.text for token in nlp(text) if token.is_title],\n",
    "        'word_frequencies': [token.text for token in nlp(text)]\n",
    "    }\n",
    "\n",
    "# Define a function transformer to apply the feature extraction function to each document in the dataset\n",
    "extract_features = FunctionTransformer(lambda texts: [analyze_single(text) for text in texts])\n",
    "\n",
    "# Define a pipeline that combines the feature extraction and vectorization steps\n",
    "pipeline = Pipeline([\n",
    "    ('extract_features', extract_features),\n",
    "    ('vectorize', DictVectorizer()),\n",
    "])\n",
    "\n",
    "# Apply the pipeline to the training data to extract and vectorize the features\n",
    "X_train = pipeline.fit_transform(train_texts[0:10000])\n",
    "y_train = train_labels[0:10000]\n",
    "\n",
    "# Print the feature matrix\n",
    "print(X_train.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A bunch of helper functions for analysis of text, utilized by an effective main.\n",
    "The plan is to use it for feature engineering / quantitative insights on code.\n",
    "\"\"\"\n",
    "\n",
    "import string\n",
    "\n",
    "def count_double_spaces(text):\n",
    "    \"\"\"\n",
    "    number of occurrences of double spaces in the given text (thanks Dr. Han!)\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    count = sum([1 for w in words if '  ' in w])\n",
    "    return count\n",
    "\n",
    "  \n",
    "def count_punctuation(text):\n",
    "    \"\"\"\n",
    "    number of punctuation marks in the given text.\n",
    "    \"\"\"\n",
    "    return sum([1 for c in text if c in string.punctuation])\n",
    "\n",
    "\n",
    "def calculate_punct_density(text):\n",
    "    \"\"\"\n",
    "    punctuation density (# of punctuation / len of text)\n",
    "    \"\"\"\n",
    "    total_chars = len(text)\n",
    "    punct_count = count_punctuation(text)\n",
    "    return 0 if total_chars == 0 else punct_count / total_chars\n",
    "\n",
    "\n",
    "def count_exclamation_marks(text):\n",
    "    \"\"\"\n",
    "    number of exclamation marks\n",
    "    \"\"\"\n",
    "    return text.count('!')\n",
    "\n",
    "\n",
    "def count_question_marks(text):\n",
    "    \"\"\"\n",
    "    number of question marks\n",
    "    \"\"\"\n",
    "    return text.count('?')\n",
    "\n",
    "\n",
    "def calculate_exclamation_ratio(text):\n",
    "    \"\"\"\n",
    "    ratio of exclamation marks to the total number of punctuation marks\n",
    "    \"\"\"\n",
    "    punct_count = count_punctuation(text)\n",
    "    exclamation_count = count_exclamation_marks(text)\n",
    "    return 0 if punct_count == 0 else exclamation_count / punct_count\n",
    "\n",
    "\n",
    "def calculate_question_ratio(text):\n",
    "    \"\"\"\n",
    "    ratio of question marks to the total number of punctuation marks\n",
    "    \"\"\"\n",
    "    punct_count = count_punctuation(text)\n",
    "    question_count = count_question_marks(text)\n",
    "    return 0 if punct_count == 0 else question_count / punct_count\n",
    "\n",
    "\n",
    "def analyze(text):\n",
    "    \"\"\"\n",
    "    returns everything\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"punct_count\": count_punctuation(text),\n",
    "        \"punct_density\": calculate_punct_density(text),\n",
    "        \"exclamation_ratio\": calculate_exclamation_ratio(text),\n",
    "        \"question_ratio\": calculate_question_ratio(text),\n",
    "        \"double_spaces\": count_double_spaces(text),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.45267457992908894\n"
     ]
    }
   ],
   "source": [
    "def extract_features(texts):\n",
    "    \"\"\"\n",
    "    extract features from a list of texts using the analyze function\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    for text in texts:\n",
    "        analysis = analyze(text)\n",
    "        features.append([\n",
    "            analysis[\"punct_count\"],\n",
    "            analysis[\"punct_density\"],\n",
    "            analysis[\"exclamation_ratio\"],\n",
    "            analysis[\"question_ratio\"],\n",
    "            analysis[\"double_spaces\"],\n",
    "        ])\n",
    "    return np.array(features)\n",
    "\n",
    "def train_model(X_train, y_train):\n",
    "    \"\"\"\n",
    "    trains a Multinomial Naive Bayes model on the training data\n",
    "    \"\"\"\n",
    "    model = MultinomialNB()\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    evaluates the accuracy of the model on the testing data\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# extract features from the training data\n",
    "X_train = extract_features(train_texts)\n",
    "y_train = train_labels\n",
    "\n",
    "# train the model\n",
    "model = train_model(X_train, y_train)\n",
    "\n",
    "# extract features from the testing data\n",
    "X_test = extract_features(test_texts)\n",
    "y_test = test_labels\n",
    "\n",
    "# evaluate the model on the testing data\n",
    "accuracy = evaluate_model(model, X_test, y_test)\n",
    "\n",
    "# print the accuracy\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6577770926468322\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "\n",
    "\n",
    "# create a pipeline that first extracts hand-crafted features,\n",
    "# then vectorizes the texts using CountVectorizer, and finally applies\n",
    "# feature selection using SelectKBest with chi2 and scaling using StandardScaler,\n",
    "# before training a Multinomial Naive Bayes classifier\n",
    "pipeline = Pipeline([\n",
    "    ('extract_features', FeatureUnion([\n",
    "        ('handcrafted', FunctionTransformer(extract_features)),\n",
    "        ('vectorize', CountVectorizer()),\n",
    "    ])),\n",
    "    ('feature_selection', SelectKBest(chi2, k=1000)),\n",
    "    ('scaling', StandardScaler(with_mean=False)),\n",
    "    ('classification', MultinomialNB()),\n",
    "])\n",
    "\n",
    "# train the model\n",
    "pipeline.fit(train_texts, train_labels)\n",
    "\n",
    "# evaluate the model on the testing data\n",
    "y_pred = pipeline.predict(test_texts)\n",
    "accuracy = accuracy_score(test_labels, y_pred)\n",
    "\n",
    "# print the accuracy\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
