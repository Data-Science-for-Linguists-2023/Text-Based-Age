{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Age Analysis of Blog Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>14,May,2004</td>\n",
       "      <td>Info has been found (+/- 100 pages,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>13,May,2004</td>\n",
       "      <td>These are the team members:   Drewe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>12,May,2004</td>\n",
       "      <td>In het kader van kernfusie op aarde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>12,May,2004</td>\n",
       "      <td>testing!!!  testing!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>11,June,2004</td>\n",
       "      <td>Thanks to Yahoo!'s Toolbar I can ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id gender  age              topic      sign          date  \\\n",
       "0  2059027   male   15            Student       Leo   14,May,2004   \n",
       "1  2059027   male   15            Student       Leo   13,May,2004   \n",
       "2  2059027   male   15            Student       Leo   12,May,2004   \n",
       "3  2059027   male   15            Student       Leo   12,May,2004   \n",
       "4  3581210   male   33  InvestmentBanking  Aquarius  11,June,2004   \n",
       "\n",
       "                                                text  \n",
       "0             Info has been found (+/- 100 pages,...  \n",
       "1             These are the team members:   Drewe...  \n",
       "2             In het kader van kernfusie op aarde...  \n",
       "3                   testing!!!  testing!!!            \n",
       "4               Thanks to Yahoo!'s Toolbar I can ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('../../../data/blogtext 2.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>text</th>\n",
       "      <th>age_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>Info has been found (+/- 100 pages, and 4.5 MB...</td>\n",
       "      <td>10s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>These are the team members:   Drewes van der L...</td>\n",
       "      <td>10s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>In het kader van kernfusie op aarde:  MAAK JE ...</td>\n",
       "      <td>10s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>testing!!!  testing!!!</td>\n",
       "      <td>10s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33</td>\n",
       "      <td>Thanks to Yahoo!'s Toolbar I can now 'capture'...</td>\n",
       "      <td>30s</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age                                               text age_category\n",
       "0   15  Info has been found (+/- 100 pages, and 4.5 MB...          10s\n",
       "1   15  These are the team members:   Drewes van der L...          10s\n",
       "2   15  In het kader van kernfusie op aarde:  MAAK JE ...          10s\n",
       "3   15                             testing!!!  testing!!!          10s\n",
       "4   33  Thanks to Yahoo!'s Toolbar I can now 'capture'...          30s"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'] = data['text'].str.strip()\n",
    "data = data.drop(['id', 'gender', 'topic', 'sign', 'date'], axis = 1)\n",
    "data['age_category'] = pd.cut(data['age'], bins=[10, 19, 29, 39], labels=['10s', '20s', '30s'])\n",
    "data.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now more or less in it's final form."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier and Vectorizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the main vectorizer and classifier. We might make changes to these, but for now, we're keeping it all in one place for covenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def train_classifier(texts, labels, ngrams):\n",
    "    # create a CountVectorizer with character-based bigrams\n",
    "    vectorizer = CountVectorizer(analyzer='char', ngram_range=ngrams, lowercase=False, stop_words=None)\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    # train the classifier and return it\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(X, labels)\n",
    "    return clf, vectorizer\n",
    "\n",
    "def train_word_classifier(texts, labels):\n",
    "    # create a CountVectorizer with words\n",
    "    vectorizer = CountVectorizer(lowercase=False, stop_words=None)\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    # train the classifier and return it\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(X, labels)\n",
    "    return clf, vectorizer\n",
    "\n",
    "def predict_age_group(classifier, vectorizer, new_text):\n",
    "    # take in a classifier as input and return the prediction\n",
    "    new_X = vectorizer.transform([new_text])\n",
    "    predicted_age_group = classifier.predict(new_X)\n",
    "    return predicted_age_group\n",
    "\n",
    "def evaluate_classifier(classifier, vectorizer, test_texts, test_labels):\n",
    "    # transform the test data\n",
    "    X_test = vectorizer.transform(test_texts)\n",
    "    # predict the age group and return score\n",
    "    predicted_age_groups = classifier.predict(X_test)\n",
    "    return accuracy_score(test_labels, predicted_age_groups)\n",
    "\n",
    "\n",
    "def analyze_features(N, clf, vec):\n",
    "    feature_names = list(vec.vocabulary_.keys())\n",
    "    log_prob = clf.feature_log_prob_\n",
    "    top_N_features = []\n",
    "    for i in range(clf.classes_.shape[0]):\n",
    "        top_N_indices = log_prob[i].argsort()[::-1][:N]\n",
    "        top_N_features.extend([feature_names[idx] for idx in top_N_indices])\n",
    "    print(\"Top {} most significant textual features:\".format(N))\n",
    "    print(top_N_features)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the analysis in the `../draft/` folder, we have determined that some preprocessing helps, but not all the way (with lowercase, stemming, lemmatizing, etc.). This also helps to an extent to get the non-english text out, or at least, non-english characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_texts(texts):\n",
    "    \"\"\"\n",
    "    Cleans the given text by removing non-English characters and punctuation.\n",
    "    \"\"\"\n",
    "    # Keep only English characters and punctuation\n",
    "    text = [re.sub(r'[^a-zA-Z\\s\\.,?!]', '', text) for text in texts]\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed = preprocess_texts(list(data['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(data['age_category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10s', '10s', '10s']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0:3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a function to drop text that's too short, and if there aren't any alphabetical bits in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def drop_short_text(text, labels, min_length=5):\n",
    "    \"\"\"\n",
    "    Drop elements from `text` and `labels` lists where the length of `text` is less than `min_length`\n",
    "    or where `text` only contains punctuation or is empty\n",
    "    \"\"\"\n",
    "    filtered_text = []\n",
    "    filtered_labels = []\n",
    "    for i in range(len(text)):\n",
    "        if len(text[i]) < min_length or all(char in string.punctuation or char.isspace() for char in text[i]):\n",
    "            # check if text is too short or only contains punctuation or whitespace\n",
    "            continue  # skip to next iteration if true\n",
    "        if not any(c.isalpha() for c in text[i]):\n",
    "            # check if text has at least one alphabet character\n",
    "            continue  # skip to next iteration if false\n",
    "        filtered_text.append(text[i])\n",
    "        filtered_labels.append(labels[i])\n",
    "    return filtered_text, filtered_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "681284\n",
      "681284\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text, labels = drop_short_text(preprocessed, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "676839\n",
      "676839\n"
     ]
    }
   ],
   "source": [
    "print(len(text))\n",
    "print(len(labels))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, looks like we filtered out a good bit. Now, let's split it into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(text, labels, test_size=0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification and Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6376470066780923\n",
      "Top 10 most significant textual features:\n",
      "['oratote', 'comreligion', 'notquiteright', 'holderjoint', 'Muhahahhaha', 'Casacommunitas', 'slurtalking', 'smilingandflirting', 'LisaaaaAAA', 'Reng', 'oratote', 'comreligion', 'notquiteright', 'holderjoint', 'Muhahahhaha', 'smilingandflirting', 'Reng', 'Casacommunitas', 'slurtalking', 'aloofly', 'oratote', 'comreligion', 'notquiteright', 'holderjoint', 'smilingandflirting', 'Muhahahhaha', 'Reng', 'Casacommunitas', 'aloofly', 'slurtalking', 'oratote', 'comreligion', 'notquiteright', 'holderjoint', 'smilingandflirting', 'Muhahahhaha', 'Reng', 'Casacommunitas', 'aloofly', 'CastorKing']\n"
     ]
    }
   ],
   "source": [
    "# Train a classifier on the training data\n",
    "clf, vectorizer = train_word_classifier(train_texts, train_labels)\n",
    "\n",
    "# Evaluate the classifier on the test data\n",
    "accuracy = evaluate_classifier(clf, vectorizer, test_texts, test_labels)\n",
    "\n",
    "# Print the accuracy score\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "analyze_features(10, clf, vectorizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.45596448200460965\n",
      "Top 10 most significant textual features:\n",
      "['oN', 'rs', 'Kg', 'Uu', 'ti', '\\xa0,', 'iR', ',A', 'wA', 'il', 'oN', 'rs', 'Kg', 'Uu', 'ti', '\\xa0,', 'iR', 'wA', ',A', 'il', 'oN', 'rs', 'Uu', 'Kg', 'ti', '\\xa0,', 'iR', 'wA', ',A', 'sT', 'oN', 'rs', 'Uu', 'ti', '\\xa0,', 'Kg', 'iR', 'wA', ',A', 'sT']\n"
     ]
    }
   ],
   "source": [
    "# Train a classifier on the training data\n",
    "clf, vectorizer = train_classifier(train_texts, train_labels, (2,2))\n",
    "\n",
    "# Evaluate the classifier on the test data\n",
    "accuracy = evaluate_classifier(clf, vectorizer, test_texts, test_labels)\n",
    "\n",
    "# Print the accuracy score\n",
    "print(\"Accuracy:\", accuracy)\n",
    "analyze_features(10, clf, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4816721824951244\n",
      "Top 10 most significant textual features:\n",
      "['sht', 'MKM', 'YSk', 'i l', 'xTM', 'XNx', 'nge', 'Wxq', 'jvh', '.bd', 'sht', 'MKM', 'YSk', 'xTM', 'i l', 'XNx', 'Wxq', 'nge', 'jvh', '.bd', 'sht', 'MKM', 'YSk', 'xTM', 'i l', 'XNx', 'Wxq', 'nge', 'jvh', '.bd', 'sht', 'MKM', 'YSk', 'xTM', 'i l', 'XNx', 'nge', 'Wxq', 'jvh', '.bd']\n"
     ]
    }
   ],
   "source": [
    "# Train a classifier on the training data\n",
    "clf, vectorizer = train_classifier(train_texts, train_labels, (3,3))\n",
    "\n",
    "# Evaluate the classifier on the test data\n",
    "accuracy = evaluate_classifier(clf, vectorizer, test_texts, test_labels)\n",
    "\n",
    "# Print the accuracy score\n",
    "print(\"Accuracy:\", accuracy)\n",
    "analyze_features(10, clf, vectorizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5296968264286981\n",
      "Top 10 most significant textual features:\n",
      "['..eh', 'czx ', 'rUBI', 'earg', 'zzto', 'adhe', 'iaen', 'pami', ' Eee', 'stwi', '..eh', 'czx ', 'rUBI', 'earg', 'zzto', 'adhe', 'pami', 'iaen', ' Eee', 'OfIn', '..eh', 'czx ', 'rUBI', 'earg', 'zzto', 'adhe', 'pami', 'iaen', ' Eee', 'tor!', '..eh', 'czx ', 'rUBI', 'earg', 'zzto', 'adhe', 'pami', 'iaen', ' Eee', 'tor!']\n"
     ]
    }
   ],
   "source": [
    "# Train a classifier on the training data\n",
    "clf, vectorizer = train_classifier(train_texts, train_labels, (4,4))\n",
    "\n",
    "# Evaluate the classifier on the test data\n",
    "accuracy = evaluate_classifier(clf, vectorizer, test_texts, test_labels)\n",
    "\n",
    "# Print the accuracy score\n",
    "print(\"Accuracy:\", accuracy)\n",
    "analyze_features(10, clf, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5787409136575853\n",
      "Top 10 most significant textual features:\n",
      "['ayabu', '...Ng', 'ilor ', 'hyWeb', 't. .M', 'Anhs ', 'Duuuu', 'mG C ', 'ada?U', '?, mu', 'ayabu', '...Ng', 'ilor ', 'hyWeb', 'Duuuu', 't. .M', 'm dOn', 'mG C ', 'zs la', 'Anhs ', 'ayabu', '...Ng', 'ilor ', 'hyWeb', 'Duuuu', 't. .M', 'm dOn', 'zs la', 'mG C ', 'FOS G', 'ayabu', '...Ng', 'ilor ', 'hyWeb', 'Duuuu', 'm dOn', 't. .M', 's Zen', 'zs la', 'mG C ']\n"
     ]
    }
   ],
   "source": [
    "# Train a classifier on the training data\n",
    "clf, vectorizer = train_classifier(train_texts, train_labels, (5,5))\n",
    "\n",
    "# Evaluate the classifier on the test data\n",
    "accuracy = evaluate_classifier(clf, vectorizer, test_texts, test_labels)\n",
    "\n",
    "# Print the accuracy score\n",
    "print(\"Accuracy:\", accuracy)\n",
    "analyze_features(10, clf, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6175314697712901\n",
      "Top 10 most significant textual features:\n",
      "['A. Thi', '! daik', 'g de a', 'kur. D', 'oesnt?', 'ohn. I', 'ue, Pu', 'ame, h', 'ND Bel', 'ythmSe', 'A. Thi', 'g de a', '! daik', 'kur. D', 'ohn. I', 'ierenc', 'ue, Pu', 'cccbcc', 'di bid', 'lxsimp', 'A. Thi', 'kur. D', 'g de a', '! daik', 'ierenc', 'ohn. I', 'di bid', 'cccbcc', ' . Aid', 'anma B', 'A. Thi', 'kur. D', 'g de a', '! daik', 'ierenc', ' i tag', 'di bid', 'cccbcc', 'lxsimp', 'ohn. I']\n"
     ]
    }
   ],
   "source": [
    "# Train a classifier on the training data\n",
    "clf, vectorizer = train_classifier(train_texts, train_labels, (6,6))\n",
    "\n",
    "# Evaluate the classifier on the test data\n",
    "accuracy = evaluate_classifier(clf, vectorizer, test_texts, test_labels)\n",
    "\n",
    "# Print the accuracy score\n",
    "print(\"Accuracy:\", accuracy)\n",
    "analyze_features(10, clf, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6405797529696826\n",
      "Top 10 most significant textual features:\n",
      "['ason, S', 'ofit.As', 'Last xm', 'n. Nana', 'ch warm', '. NO BR', 'cate al', 'nse aff', 'RA! din', 'th aspe', 'ch warm', 'RA! din', 'ason, S', 'cate al', 'nse aff', 'ofit.As', 'nn Proj', ' eh!! d', '. NO BR', 'wns his', 'ch warm', 'nn Proj', ' eh!! d', 'RA! din', 'wns his', 'cate al', 'nse aff', 'ason, S', 'ofit.As', '. NO BR', 'ch warm', 'RA! din', 'th aspe', 'cate al', 'nse aff', 'nn Proj', ' eh!! d', 'ason, S', 'c n na ', '. NO BR']\n"
     ]
    }
   ],
   "source": [
    "# Train a classifier on the training data\n",
    "clf, vectorizer = train_classifier(train_texts, train_labels, (7,7))\n",
    "\n",
    "# Evaluate the classifier on the test data\n",
    "accuracy = evaluate_classifier(clf, vectorizer, test_texts, test_labels)\n",
    "\n",
    "# Print the accuracy score\n",
    "print(\"Accuracy:\", accuracy)\n",
    "analyze_features(10, clf, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
