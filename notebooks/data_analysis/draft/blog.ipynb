{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Age Classificaion/Analysis of Text and Age\n",
    "\n",
    "Let's actually start with a classification model, gather the results, and then investigate.\n",
    "\n",
    "Here's the plan for the upcoming work in the notebook --\n",
    "\n",
    "- CountVectorizer (vector representation of text) on N-grams\n",
    "- Train a Naive Bayes Classifier\n",
    "- Look at most significant features\n",
    "- Form hypotheses and iterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>14,May,2004</td>\n",
       "      <td>Info has been found (+/- 100 pages,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>13,May,2004</td>\n",
       "      <td>These are the team members:   Drewe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>12,May,2004</td>\n",
       "      <td>In het kader van kernfusie op aarde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>12,May,2004</td>\n",
       "      <td>testing!!!  testing!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>11,June,2004</td>\n",
       "      <td>Thanks to Yahoo!'s Toolbar I can ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id gender  age              topic      sign          date  \\\n",
       "0  2059027   male   15            Student       Leo   14,May,2004   \n",
       "1  2059027   male   15            Student       Leo   13,May,2004   \n",
       "2  2059027   male   15            Student       Leo   12,May,2004   \n",
       "3  2059027   male   15            Student       Leo   12,May,2004   \n",
       "4  3581210   male   33  InvestmentBanking  Aquarius  11,June,2004   \n",
       "\n",
       "                                                text  \n",
       "0             Info has been found (+/- 100 pages,...  \n",
       "1             These are the team members:   Drewe...  \n",
       "2             In het kader van kernfusie op aarde...  \n",
       "3                   testing!!!  testing!!!            \n",
       "4               Thanks to Yahoo!'s Toolbar I can ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('../../data/blogtext 2.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>text</th>\n",
       "      <th>age_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>Info has been found (+/- 100 pages, and 4.5 MB...</td>\n",
       "      <td>10s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>These are the team members:   Drewes van der L...</td>\n",
       "      <td>10s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>In het kader van kernfusie op aarde:  MAAK JE ...</td>\n",
       "      <td>10s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>testing!!!  testing!!!</td>\n",
       "      <td>10s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33</td>\n",
       "      <td>Thanks to Yahoo!'s Toolbar I can now 'capture'...</td>\n",
       "      <td>30s</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age                                               text age_category\n",
       "0   15  Info has been found (+/- 100 pages, and 4.5 MB...          10s\n",
       "1   15  These are the team members:   Drewes van der L...          10s\n",
       "2   15  In het kader van kernfusie op aarde:  MAAK JE ...          10s\n",
       "3   15                             testing!!!  testing!!!          10s\n",
       "4   33  Thanks to Yahoo!'s Toolbar I can now 'capture'...          30s"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'] = data['text'].str.strip()\n",
    "data = data.drop(['id', 'gender', 'topic', 'sign', 'date'], axis = 1)\n",
    "data['age_category'] = pd.cut(data['age'], bins=[10, 19, 29, 39], labels=['10s', '20s', '30s'])\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def train_classifier(texts, labels, ngrams):\n",
    "    # create a CountVectorizer with character-based bigrams\n",
    "    vectorizer = CountVectorizer(analyzer='char', ngram_range=ngrams, lowercase=False, stop_words=None)\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    # train the classifier and return it\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(X, labels)\n",
    "    return clf, vectorizer\n",
    "\n",
    "def train_word_classifier(texts, labels):\n",
    "    # create a CountVectorizer with words\n",
    "    vectorizer = CountVectorizer(lowercase=False, stop_words=None)\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    # train the classifier and return it\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(X, labels)\n",
    "    return clf, vectorizer\n",
    "\n",
    "def predict_age_group(classifier, vectorizer, new_text):\n",
    "    # take in a classifier as input and return the prediction\n",
    "    new_X = vectorizer.transform([new_text])\n",
    "    predicted_age_group = classifier.predict(new_X)\n",
    "    return predicted_age_group\n",
    "\n",
    "def evaluate_classifier(classifier, vectorizer, test_texts, test_labels):\n",
    "    # transform the test data\n",
    "    X_test = vectorizer.transform(test_texts)\n",
    "    # predict the age group and return score\n",
    "    predicted_age_groups = classifier.predict(X_test)\n",
    "    return accuracy_score(test_labels, predicted_age_groups)\n",
    "\n",
    "\n",
    "def analyze_features(N, clf, vec):\n",
    "    feature_names = list(vec.vocabulary_.keys())\n",
    "    log_prob = clf.feature_log_prob_\n",
    "    top_N_features = []\n",
    "    for i in range(clf.classes_.shape[0]):\n",
    "        top_N_indices = log_prob[i].argsort()[::-1][:N]\n",
    "        top_N_features.extend([feature_names[idx] for idx in top_N_indices])\n",
    "    print(\"Top {} most significant textual features:\".format(N))\n",
    "    print(top_N_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(list(data['text']), list(data['age_category']), test_size=0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if preprocessing matters. But first, let's get a bench mark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a classifier on the training data\n",
    "clf, vectorizer = train_word_classifier(train_texts, train_labels)\n",
    "\n",
    "# Evaluate the classifier on the test data\n",
    "accuracy = evaluate_classifier(clf, vectorizer, test_texts, test_labels)\n",
    "\n",
    "# Print the accuracy score\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "analyze_features(10, clf, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4683135545329781\n"
     ]
    }
   ],
   "source": [
    "# Train a classifier on the training data\n",
    "clf, vectorizer = train_classifier(train_texts, train_labels, (2,2))\n",
    "\n",
    "# Evaluate the classifier on the test data\n",
    "accuracy = evaluate_classifier(clf, vectorizer, test_texts, test_labels)\n",
    "\n",
    "# Print the accuracy score\n",
    "print(\"Accuracy:\", accuracy)\n",
    "analyze_features(10, clf, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_texts(texts):\n",
    "    preprocessed_texts = []\n",
    "    \n",
    "    for text in texts:\n",
    "        # lowercase the text\n",
    "        text = text.lower()\n",
    "        \n",
    "        # remove URLs and email addresses\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+|ftp\\S+|@\\S+', '', text, flags=re.MULTILINE)\n",
    "        \n",
    "        # remove non-alphanumeric characters except for spaces\n",
    "        text = re.sub(r'[^a-z\\s]', '', text)\n",
    "        \n",
    "        # remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        # re-join the tokens into a single string\n",
    "        preprocessed_text = text\n",
    "        \n",
    "        preprocessed_texts.append(preprocessed_text)\n",
    "    \n",
    "    return preprocessed_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_train, preprocess_test = preprocess_texts(train_texts), preprocess_texts(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6405689248992712\n"
     ]
    }
   ],
   "source": [
    "# Train a classifier on the training data\n",
    "clf, vectorizer = train_word_classifier(preprocessed_train, train_labels)\n",
    "\n",
    "# Evaluate the classifier on the test data\n",
    "accuracy = evaluate_classifier(clf, vectorizer, preprocess_test, test_labels)\n",
    "\n",
    "# Print the accuracy score\n",
    "print(\"Accuracy:\", accuracy)\n",
    "analyze_features(10, clf, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['urllink center for american progress the progress report page tenet targets white house in a major speech this morning addressing the failure to find wmd in iraq cia director george tenet said the intelligence community never told the white house that iraq was an imminent threat to america a stunning blow to the white house',\n",
       " 'probably to spend the rest of the day cod maybe ill blog tomorrow',\n",
       " 'p stands for the holy grill went to play football with isaiah khairul daryl and the crazy nut who just runs on and on by his own before khairul cuts him down with more grace than a swan and oks is just mugging and mugging and mugging away if isaiah can run until the cows return home for the sixtieth time at a go oks can mug twice as much and exams argh i hate exams kenn just makes me so effing jealous cos he just studies so much you could smell his studiousness lingering in the air like a very long very loud and above all very disgusting fart bonus points for a squisssshhhhhbrapppppp accompanying band if you are into queer and sometimes lazy brit music try stereolab its not anything as brilliant as their work but its more electronic goodness and more queerness and so forth cy says its the soundtrack of my life it sizzles it fuzzes and pulses it is naughty and playful and buzzes with blatant transgress of the immaculateness of prepuberty and yes thats why i am sometimes filled with a tremendous envy and respect for cys command of my favourite language and sometimes filled with a certain desire to choke her with a nylon cord because she can sometimes sound like she is one of those people who flip through a thick thesaurus just to get a sophisticated word to use when she is to the best of my understanding nothing of the kind and yes runon sentences because i cannot remember what the actual term is and bad grammar is good or bad',\n",
       " 'this is a program where you save animals peoplethe world we need more people saving the world feed dogs veggies not meat because what meat does is make dogs more savage and mean you can also start like a little liter program were you pick up liter from everywhere you can also start adoption agencies because alot of animals are killed either because they are abandond or nobody adopts them or stray so be cool and help the world',\n",
       " 'so we come home after the concert and amys icecream we were feeling great due to work we were in two cars and my sweetie stopped for at the mailbox on his way to the house as we enter the house i asked him what we got in the mail he says calmly something from those accounting people my heart stops is this the letter i am waiting for he hands me two envelopes two two has to be good right i open the thinner envelope first the one the size of the we need more information letters to my surprise i read the first line congratulations your application of intent is approved woo hooo the second envelop is my form to send in saying when i want to test for each section this means that i can test for the cpa this means that they have all of my paperwork and approved of my education this means that i have to take the next step in order to not stress over something that is not even there i told myself that i could not think about when to study and test until i received the approval letter now it is here it could not have come on a better day after the sarah concert however i have papers to write this weekend and cannot think about the cpa exam yet but i am so excited i am so proud i am so scared']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.47052995442435985\n"
     ]
    }
   ],
   "source": [
    "# Train a classifier on the training data\n",
    "clf, vectorizer = train_classifier(preprocessed_train, train_labels, (2,2))\n",
    "\n",
    "# Evaluate the classifier on the test data\n",
    "accuracy = evaluate_classifier(clf, vectorizer, preprocess_test, test_labels)\n",
    "\n",
    "# Print the accuracy score\n",
    "print(\"Accuracy:\", accuracy)\n",
    "analyze_features(10, clf, vectorizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6405689248992712\n"
     ]
    }
   ],
   "source": [
    "# Train a classifier on the training data\n",
    "clf, vectorizer = train_word_classifier(preprocessed_train, train_labels)\n",
    "\n",
    "# Evaluate the classifier on the test data\n",
    "accuracy = evaluate_classifier(clf, vectorizer, preprocess_test, test_labels)\n",
    "\n",
    "# Print the accuracy score\n",
    "print(\"Accuracy:\", accuracy)\n",
    "analyze_features(10, clf, vectorizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finished preprocessing vs raw data. let's see what features were learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train a classifier on the training data\n",
    "clf, vectorizer = train_classifier(train_texts, train_labels, (3,3))\n",
    "\n",
    "# Evaluate the classifier on the test data\n",
    "accuracy = evaluate_classifier(clf, vectorizer, test_texts, test_labels)\n",
    "\n",
    "# Print the accuracy score\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we have a lot to improve upon. Let's start with using words instead of N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "from scripts import classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6310868432447507\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train_word_classifier(texts, labels):\n",
    "    # create a CountVectorizer with words\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    # train the classifier and return it\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(X, labels)\n",
    "    return clf, vectorizer\n",
    "\n",
    "# Train a classifier on the training data\n",
    "clf, vectorizer = train_word_classifier(train_texts, train_labels)\n",
    "\n",
    "# Evaluate the classifier on the test data\n",
    "accuracy = evaluate_classifier(clf, vectorizer, test_texts, test_labels)\n",
    "\n",
    "# Print the accuracy score\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay much better, I guess I'll have to read up on where and when to use N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
